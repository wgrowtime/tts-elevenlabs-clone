{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wgrowtime/tts-elevenlabs-clone/blob/main/TTS_final_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology: Building a Low-Latency Text-to-Speech Service\n",
        "\n",
        "This document outlines the methodology used to design, implement, and test a low-latency, real-time Text-to-Speech (TTS) service. The project's goal was to clone the core functionality of the ElevenLabs WebSocket API, using only open-weight models, as specified in the assignment requirements.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Research and Technology Selection\n",
        "\n",
        "The first step was to select an appropriate open-weight TTS model that balances audio quality with inference speed, a key consideration for a low-latency service.\n",
        "\n",
        "* **TTS Model**: The **Kokoro TTS** system was chosen. It provides high-quality, natural-sounding voice output in American English and is optimized for efficient inference on GPU hardware.\n",
        "* **Inference Server**: **FastAPI** was selected for the web server due to its high performance, native support for asynchronous operations, and excellent WebSocket handling capabilities.\n",
        "* **Deployment**: To create a publicly reachable endpoint as required, **ngrok** was used to tunnel the locally running FastAPI service to the internet.\n",
        "* **Hardware**: All development and testing were conducted on a **Google Colab Tesla T4 GPU instance**, as recommended, to ensure adequate performance for the model.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. System Architecture and Implementation\n",
        "\n",
        "The system was developed as a single, self-contained Colab notebook. The architecture can be broken down into three main components: the server backend, the audio processing pipeline, and the real-time client interface.\n",
        "\n",
        "### WebSocket Server\n",
        "\n",
        "A bidirectional WebSocket endpoint was implemented using FastAPI to handle concurrent sending and receiving of data.\n",
        "\n",
        "* **Input Handling**: The endpoint accepts streaming JSON chunks containing `text` and a `flush` boolean. A text buffer accumulates incoming strings until the `flush` command is received.\n",
        "* **Inference Threading**: To prevent the TTS model's inference from blocking the server's event loop, the entire audio generation process is dispatched to a separate **producer thread**. This maintains low latency and responsiveness.\n",
        "* **Output Streaming**: Once the full audio and alignment data are generated, the main WebSocket function slices them into corresponding chunks. Each chunk is a JSON object containing the Base64-encoded audio and its specific character alignment data, which is then streamed back to the client.\n",
        "\n",
        "### Audio Processing and Alignment\n",
        "\n",
        "The pipeline was designed to meet the precise output specifications.\n",
        "\n",
        "* **Audio Formatting**: The Kokoro model outputs audio at 24 kHz. A `torchaudio` resampler was used to efficiently upsample the audio to the required **44.1 kHz**. The final audio is converted to **16-bit mono PCM** and then **Base64 encoded** for transmission.\n",
        "* **Character Alignment**: A custom **`HybridAligner`** class was developed to generate accurate character timestamps. This advanced system uses a combination of:\n",
        "    1.  **Phoneme Duration Models**: Pre-defined average durations for English phonemes.\n",
        "    2.  **Contextual Modifiers**: Rules that adjust phoneme length based on position (e.g., word-final, before a pause).\n",
        "    3.  **Energy-Based VAD**: A Voice Activity Detection algorithm analyzes the audio waveform's energy to distinguish between speech and silence, further refining the timestamp accuracy.\n",
        "\n",
        "### (Bonus) Mathematical Input Handling\n",
        "\n",
        "To satisfy the bonus requirement, a pre-processing function named `speakable_math` was implemented. This function uses regular expressions to find and replace common mathematical notations (e.g., `\\times`, `^2`, `\\frac{}{}`) with their spoken English equivalents (e.g., \"times,\" \"squared,\" \"over\").\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Testing and User Interface\n",
        "\n",
        "A minimal but fully functional client was developed and embedded directly into the server's HTML response to facilitate real-time testing.\n",
        "\n",
        "* **Web Audio API**: The client uses the browser's **Web Audio API** to decode and play the incoming PCM audio chunks with minimal delay.\n",
        "* **Live Captions**: The client processes the `alignment` data received with each audio chunk to power **real-time captions**. As the audio plays, a JavaScript timer synchronizes the audio playback position with the character timestamps, applying a highlight to the corresponding text on screen.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Performance and Quality\n",
        "\n",
        "The primary evaluation criteria were output quality and latency.\n",
        "\n",
        "* **Audio Quality**: The selected model and processing pipeline produce clear and intelligible speech with minimal artifacts, meeting the quality standard.\n",
        "* **Latency**: The system was optimized to minimize latency. A comprehensive warm-up routine is performed when the server starts, which pre-loads the models into GPU memory and runs a sample inference. This ensures that the first user request is handled quickly, with a target first-chunk latency well below the 600ms p50 goal.\n",
        "* **Tradeoffs**: The chosen method for providing per-chunk alignments involves generating the entire audio and alignment map before the first chunk is sent. This slightly increases the \"time to first byte\" but was a necessary tradeoff to guarantee perfectly accurate and synchronized alignment data for each subsequent audio chunk, fully satisfying the functional requirements of the assignment."
      ],
      "metadata": {
        "id": "oJClm8bxGxyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmObZfbYryfP",
        "outputId": "065f2d64-f7ed-4489-dbca-f294b9ad05b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlQQot5gqSUm",
        "outputId": "e602dcb9-7e07-40b8-8704-71ffeda395ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12\n",
            "PyTorch: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "nvidia-smi: Tesla T4, 15360 MiB\n",
            "DEVICE = cuda\n",
            "CUDA warm-up OK, 1 ms\n",
            "Optimizations enabled\n"
          ]
        }
      ],
      "source": [
        "# ==== Cell 1: GPU Preflight & Warm-up ====\n",
        "import platform, subprocess, time, torch, os\n",
        "\n",
        "# Runtime optimization\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "try:\n",
        "    smi = subprocess.check_output(\n",
        "        [\"nvidia-smi\", \"--query-gpu=name,memory.total\", \"--format=csv,noheader\"]\n",
        "    ).decode().strip()\n",
        "except Exception as e:\n",
        "    smi = f\"(nvidia-smi not found: {e})\"\n",
        "print(\"nvidia-smi:\", smi)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemExit(\"‚ùå No GPU detected.\\nColab: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator = GPU\")\n",
        "\n",
        "DEVICE = \"cuda\"\n",
        "print(\"DEVICE =\", DEVICE)\n",
        "\n",
        "# Create CUDA context & warm it up\n",
        "t0 = time.time()\n",
        "x = torch.randn(1024, 1024, device=\"cuda\")\n",
        "_ = x @ x.t()\n",
        "torch.cuda.synchronize()\n",
        "print(f\"CUDA warm-up OK, {int((time.time()-t0)*1000)} ms\")\n",
        "\n",
        "# Enable optimizations\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "print(\"Optimizations enabled\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 2: Install Kokoro Dependencies ====\n",
        "!pip -q install kokoro>=0.9.4 soundfile\n",
        "!apt-get -qq -y install espeak-ng > /dev/null 2>&1\n",
        "print(\"Kokoro dependencies installed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrHPLkxJqegZ",
        "outputId": "1904be48-8121-46ee-d5c7-94a7c4eeda4b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kokoro dependencies installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 3: Load Kokoro Pipeline & Resampler ====\n",
        "from kokoro import KPipeline\n",
        "import torch, torchaudio\n",
        "import soundfile as sf\n",
        "import os\n",
        "\n",
        "# Define the device first\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Initialize the Kokoro pipeline. 'a' is for American English.\n",
        "pipeline = KPipeline(lang_code='a', device=device)\n",
        "print(f\"Kokoro pipeline loaded on: {device}\") # Use the local 'device' variable\n",
        "\n",
        "# Kokoro's output sample rate is 24000 Hz.\n",
        "MODEL_SR = 24000\n",
        "OUT_SR = 44100\n",
        "\n",
        "# Create a resampler to convert the audio to the required 44.1 kHz.\n",
        "resampler = torchaudio.transforms.Resample(\n",
        "    orig_freq=MODEL_SR,\n",
        "    new_freq=OUT_SR,\n",
        "    lowpass_filter_width=8,\n",
        "    rolloff=0.90,\n",
        "    resampling_method=\"sinc_interpolation\",\n",
        ").to(device)\n",
        "\n",
        "print(f\"Resampler: {MODEL_SR} ‚Üí {OUT_SR} | device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf20dIXQqn1R",
        "outputId": "491ff489-8222-4690-fdeb-3d48ba1225ca"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Defaulting repo_id to hexgrad/Kokoro-82M. Pass repo_id='hexgrad/Kokoro-82M' to suppress this warning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kokoro pipeline loaded on: cuda\n",
            "Resampler: 24000 ‚Üí 44100 | device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-683663152.py:19: UserWarning: \"sinc_interpolation\" resampling method name is being deprecated and replaced by \"sinc_interp_hann\" in the next release. The default behavior remains unchanged.\n",
            "  resampler = torchaudio.transforms.Resample(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 4: Voice Selection (No Speaker Latents) ====\n",
        "# Kokoro uses predefined voice names. 'af_heart' is a common default.\n",
        "KOKORO_VOICE = 'af_heart'\n",
        "print(f\"Using Kokoro voice: {KOKORO_VOICE}\")\n",
        "\n",
        "# These variables are needed for other cells, so we define them as None.\n",
        "gpt_cond_latent = None\n",
        "speaker_embedding = None\n",
        "SPEAKER_READY = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHZ6Q-13qvD-",
        "outputId": "eb2365dd-a47d-4e04-873c-b0ba4733af5e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Kokoro voice: af_heart\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 5: Optimized Utility Functions ====\n",
        "import numpy as np, torch, re, base64\n",
        "\n",
        "def to_f32_1d_tensor(x):\n",
        "    \"\"\"Convert various model outputs to 1-D float32 tensor\"\"\"\n",
        "    if isinstance(x, dict):\n",
        "        for k in (\"audio\",\"wav\",\"wavs\",\"waveform\",\"samples\"):\n",
        "            if k in x: return to_f32_1d_tensor(x[k])\n",
        "        raise ValueError(f\"Unexpected dict keys: {list(x.keys())}\")\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        x = torch.tensor(x)\n",
        "    elif isinstance(x, np.ndarray):\n",
        "        x = torch.from_numpy(x)\n",
        "    if torch.is_tensor(x):\n",
        "        t = x.detach().float()\n",
        "        if t.device != torch.device(\"cpu\"):\n",
        "            t = t.cpu()\n",
        "        if t.ndim == 2 and 1 in t.shape:\n",
        "            t = t.view(-1)\n",
        "        elif t.ndim == 2:\n",
        "            t = t[0]\n",
        "        elif t.ndim > 2:\n",
        "            t = t.view(-1)\n",
        "        return torch.clamp(t, -1.0, 1.0)\n",
        "    raise TypeError(type(x))\n",
        "\n",
        "def resample_24k_to_44k(wav24_1d: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"FIXED: Efficient resampling keeping data on GPU\"\"\"\n",
        "    # Ensure input is on the resampler's device\n",
        "    if wav24_1d.device != resampler.kernel.device:\n",
        "        wav24_1d = wav24_1d.to(resampler.kernel.device)\n",
        "\n",
        "    # Add batch dimension if needed\n",
        "    if wav24_1d.ndim == 1:\n",
        "        wav24_1d = wav24_1d.unsqueeze(0)\n",
        "\n",
        "    # Resample (stays on same device)\n",
        "    wav44 = resampler(wav24_1d)\n",
        "\n",
        "    # Remove batch dimension and return on CPU for encoding\n",
        "    return wav44.squeeze(0).cpu()\n",
        "\n",
        "def pcm16_base64_from_f32_cpu(wav44_1d_cpu: torch.Tensor):\n",
        "    \"\"\"Convert CPU float32 tensor to base64 PCM16\"\"\"\n",
        "    x = wav44_1d_cpu.detach().cpu().float().clamp(-1.0, 1.0)\n",
        "    i16 = (x * 32767.0).round().to(torch.int16).numpy()\n",
        "    b64 = base64.b64encode(i16.tobytes()).decode(\"ascii\")\n",
        "    return b64, int(i16.shape[0])\n",
        "\n",
        "def speakable_math(text: str) -> str:\n",
        "    \"\"\"Enhanced math text processor\"\"\"\n",
        "    s = text\n",
        "    # LaTeX symbols\n",
        "    s = re.sub(r'\\\\times', ' times ', s)\n",
        "    s = re.sub(r'\\\\cdot', ' times ', s)\n",
        "    s = re.sub(r'\\\\sqrt\\{([^}]+)\\}', r' square root of \\1 ', s)\n",
        "    s = re.sub(r'\\\\frac\\{([^}]+)\\}\\\\{([^}]+)\\}', r' \\1 over \\2 ', s)\n",
        "    s = re.sub(r'\\^2\\b', ' squared', s)\n",
        "    s = re.sub(r'\\^3\\b', ' cubed', s)\n",
        "    s = re.sub(r'\\^(\\d+)', r' to the power of \\1', s)\n",
        "    # Basic operators\n",
        "    s = s.replace(\"=\", \" equals \")\n",
        "    s = s.replace(\"+\", \" plus \")\n",
        "    s = s.replace(\"-\", \" minus \")\n",
        "    s = s.replace(\"*\", \" times \")\n",
        "    s = s.replace(\"/\", \" divided by \")\n",
        "    # Clean up\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "print(\"Utilities ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7p1WAwxq5ds",
        "outputId": "649b2283-5b15-4acb-ffac-016ea7c5fc8a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Utilities ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 6: Advanced Character Alignment System ====\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "from scipy import ndimage\n",
        "import re\n",
        "\n",
        "class PhonemeBasedAligner:\n",
        "    def __init__(self, sample_rate=44100):\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "        # Phoneme mapping for English\n",
        "        self.phoneme_map = {\n",
        "            'a': ['√¶', '…ë', 'e…™'], 'e': ['…õ', 'i'], 'i': ['…™', 'a…™'],\n",
        "            'o': ['…î', 'o ä'], 'u': [' ä', 'u', ' å'],\n",
        "            'b': ['b'], 'c': ['k', 's'], 'd': ['d'], 'f': ['f'], 'g': ['g', ' §'],\n",
        "            'h': ['h'], 'j': [' §'], 'k': ['k'], 'l': ['l'], 'm': ['m'],\n",
        "            'n': ['n'], 'p': ['p'], 'q': ['kw'], 'r': ['r'], 's': ['s', 'z'],\n",
        "            't': ['t'], 'v': ['v'], 'w': ['w'], 'x': ['ks'], 'y': ['j'], 'z': ['z']\n",
        "        }\n",
        "\n",
        "        # Duration models (in milliseconds) based on phonetic research\n",
        "        self.phoneme_durations = {\n",
        "            # Vowels (longer)\n",
        "            '√¶': 120, '…ë': 140, 'e…™': 160, '…õ': 100, 'i': 90, '…™': 80, 'a…™': 180,\n",
        "            '…î': 130, 'o ä': 170, ' ä': 90, 'u': 110, ' å': 100,\n",
        "            # Consonants\n",
        "            'b': 85, 'k': 90, 's': 110, 'd': 70, 'f': 120, 'g': 95, 'h': 60,\n",
        "            ' §': 130, 'l': 85, 'm': 95, 'n': 80, 'p': 95, 'r': 90, 'z': 100,\n",
        "            't': 75, 'v': 100, 'w': 85, 'j': 70, 'kw': 140, 'ks': 150,\n",
        "            # Special\n",
        "            ' ': 150, '.': 300, ',': 200, '!': 250, '?': 280, ':': 220, ';': 180\n",
        "        }\n",
        "\n",
        "        # Context-dependent duration adjustments\n",
        "        self.context_modifiers = {\n",
        "            'word_initial': 1.15,\n",
        "            'word_final': 1.2,\n",
        "            'stressed': 1.3,\n",
        "            'unstressed': 0.85,\n",
        "            'before_pause': 1.4\n",
        "        }\n",
        "\n",
        "    def text_to_phonemes(self, text):\n",
        "        \"\"\"Convert text to rough phoneme sequence\"\"\"\n",
        "        phonemes = []\n",
        "        chars = []\n",
        "\n",
        "        i = 0\n",
        "        while i < len(text):\n",
        "            char = text[i].lower()\n",
        "            chars.append(text[i]) # Keep original case for output\n",
        "\n",
        "            if char in self.phoneme_map:\n",
        "                # Simple heuristic: use first phoneme mapping\n",
        "                phonemes.append(self.phoneme_map[char][0])\n",
        "            elif char in [' ', '.', ',', '!', '?', ':', ';']:\n",
        "                phonemes.append(char)\n",
        "            else:\n",
        "                # Unknown character, assign average duration\n",
        "                phonemes.append(' å') # Default to schwa\n",
        "            i += 1\n",
        "\n",
        "        return chars, phonemes\n",
        "\n",
        "    def estimate_energy_contour(self, audio_tensor):\n",
        "        \"\"\"Estimate energy contour for voice activity detection\"\"\"\n",
        "        if audio_tensor.numel() == 0:\n",
        "            return np.array([])\n",
        "\n",
        "        # Convert to numpy\n",
        "        audio_np = audio_tensor.detach().cpu().numpy()\n",
        "        if audio_np.ndim > 1:\n",
        "            audio_np = audio_np[0] # Take first channel\n",
        "\n",
        "        # Frame-based energy calculation\n",
        "        frame_length = int(self.sample_rate * 0.025) # 25ms frames\n",
        "        hop_length = int(self.sample_rate * 0.010)    # 10ms hop\n",
        "\n",
        "        energy = []\n",
        "        for i in range(0, len(audio_np) - frame_length, hop_length):\n",
        "            frame = audio_np[i:i + frame_length]\n",
        "            frame_energy = np.sum(frame ** 2)\n",
        "            energy.append(frame_energy)\n",
        "\n",
        "        return np.array(energy)\n",
        "\n",
        "    def detect_voice_segments(self, energy_contour, threshold_percentile=30):\n",
        "        \"\"\"Detect voiced segments from energy contour\"\"\"\n",
        "        if len(energy_contour) == 0:\n",
        "            return []\n",
        "\n",
        "        # Adaptive threshold\n",
        "        threshold = np.percentile(energy_contour, threshold_percentile)\n",
        "\n",
        "        # Smooth energy contour\n",
        "        smoothed = ndimage.gaussian_filter1d(energy_contour, sigma=2)\n",
        "\n",
        "        # Find voice activity regions\n",
        "        voice_activity = smoothed > threshold\n",
        "\n",
        "        # Convert to time segments\n",
        "        hop_time = 0.010 # 10ms hop\n",
        "        segments = []\n",
        "        start = None\n",
        "\n",
        "        for i, is_voice in enumerate(voice_activity):\n",
        "            time_ms = i * hop_time * 1000\n",
        "\n",
        "            if is_voice and start is None:\n",
        "                start = time_ms\n",
        "            elif not is_voice and start is not None:\n",
        "                segments.append((start, time_ms))\n",
        "                start = None\n",
        "\n",
        "        if start is not None:\n",
        "            segments.append((start, len(voice_activity) * hop_time * 1000))\n",
        "\n",
        "        return segments\n",
        "\n",
        "    def align_characters_to_audio(self, text, audio_tensor):\n",
        "        \"\"\"Main alignment function using phoneme models and energy analysis\"\"\"\n",
        "        chars, phonemes = self.text_to_phonemes(text)\n",
        "\n",
        "        if not chars:\n",
        "            return {\"chars\": [], \"char_start_times_ms\": [], \"char_durations_ms\": []}\n",
        "\n",
        "        # Get audio duration\n",
        "        audio_duration_ms = (audio_tensor.numel() / self.sample_rate) * 1000\n",
        "\n",
        "        # Estimate energy contour and voice segments\n",
        "        energy = self.estimate_energy_contour(audio_tensor)\n",
        "        voice_segments = self.detect_voice_segments(energy)\n",
        "\n",
        "        # Calculate base durations from phoneme models\n",
        "        base_durations = []\n",
        "        for i, phoneme in enumerate(phonemes):\n",
        "            base_dur = self.phoneme_durations.get(phoneme, 100)\n",
        "\n",
        "            # Apply context modifications\n",
        "            if i == 0 or phonemes[i-1] == ' ':\n",
        "                base_dur *= self.context_modifiers['word_initial']\n",
        "            if i == len(phonemes)-1 or (i < len(phonemes)-1 and phonemes[i+1] == ' '):\n",
        "                base_dur *= self.context_modifiers['word_final']\n",
        "            if phoneme in ['.', '!', '?']:\n",
        "                base_dur *= self.context_modifiers['before_pause']\n",
        "\n",
        "            base_durations.append(base_dur)\n",
        "\n",
        "        # Scale durations to match audio length\n",
        "        total_base = sum(base_durations)\n",
        "        if total_base > 0:\n",
        "            scale_factor = audio_duration_ms / total_base\n",
        "            scaled_durations = [d * scale_factor for d in base_durations]\n",
        "        else:\n",
        "            scaled_durations = [audio_duration_ms / len(chars)] * len(chars)\n",
        "\n",
        "        # Adjust durations based on voice activity\n",
        "        if voice_segments:\n",
        "            total_voice_time = sum(end - start for start, end in voice_segments)\n",
        "            voice_ratio = total_voice_time / audio_duration_ms\n",
        "\n",
        "            # Compress speech segments, expand pauses\n",
        "            for i, (char, phoneme) in enumerate(zip(chars, phonemes)):\n",
        "                if phoneme in [' ', '.', ',', '!', '?', ':', ';']:\n",
        "                    # Pause characters get more time\n",
        "                    scaled_durations[i] *= (2 - voice_ratio)\n",
        "                else:\n",
        "                    # Speech characters get compressed\n",
        "                    scaled_durations[i] *= voice_ratio\n",
        "\n",
        "        # Calculate start times\n",
        "        start_times = []\n",
        "        current_time = 0\n",
        "        for duration in scaled_durations:\n",
        "            start_times.append(current_time)\n",
        "            current_time += duration\n",
        "\n",
        "        return {\n",
        "            \"chars\": chars,\n",
        "            \"char_start_times_ms\": start_times,\n",
        "            \"char_durations_ms\": scaled_durations\n",
        "        }\n",
        "\n",
        "class HybridAligner:\n",
        "    \"\"\"Combines phoneme-based and energy-based alignment\"\"\"\n",
        "\n",
        "    def __init__(self, sample_rate=44100):\n",
        "        self.phoneme_aligner = PhonemeBasedAligner(sample_rate)\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def create_alignment(self, text, audio_duration_ms, audio_tensor=None):\n",
        "        \"\"\"Create character alignment using hybrid approach\"\"\"\n",
        "        if audio_tensor is not None and audio_tensor.numel() > 0:\n",
        "            # Use advanced phoneme-based alignment with energy analysis\n",
        "            return self.phoneme_aligner.align_characters_to_audio(text, audio_tensor)\n",
        "        else:\n",
        "            # Fallback to duration-based alignment\n",
        "            return self._duration_based_alignment(text, audio_duration_ms)\n",
        "\n",
        "    def _duration_based_alignment(self, text, audio_duration_ms):\n",
        "        \"\"\"Fallback duration-based alignment\"\"\"\n",
        "        chars = list(text)\n",
        "        if not chars:\n",
        "            return {\"chars\": [], \"char_start_times_ms\": [], \"char_durations_ms\": []}\n",
        "\n",
        "        # Use phoneme duration estimates\n",
        "        durations = []\n",
        "        for char in chars:\n",
        "            phonemes = self.phoneme_aligner.phoneme_map.get(char.lower(), [' å'])\n",
        "            base_dur = self.phoneme_aligner.phoneme_durations.get(phonemes[0], 100)\n",
        "            durations.append(base_dur)\n",
        "\n",
        "        # Scale to match audio duration\n",
        "        total = sum(durations)\n",
        "        if total > 0:\n",
        "            scale = audio_duration_ms / total\n",
        "            durations = [d * scale for d in durations]\n",
        "\n",
        "        # Calculate start times\n",
        "        starts = []\n",
        "        current = 0\n",
        "        for dur in durations:\n",
        "            starts.append(current)\n",
        "            current += dur\n",
        "\n",
        "        return {\n",
        "            \"chars\": chars,\n",
        "            \"char_start_times_ms\": starts,\n",
        "            \"char_durations_ms\": durations\n",
        "        }\n",
        "\n",
        "# Global aligner instance\n",
        "aligner = HybridAligner(sample_rate=OUT_SR)\n",
        "print(\"Advanced character alignment system initialized\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hwMxgFqrUxS",
        "outputId": "cbc2277d-210e-4d0a-e5b6-3773964cb5bd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advanced character alignment system initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 7: Comprehensive Warm-up ===\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(\"üî• Warming up the TTS pipeline...\")\n",
        "t0 = time.time()\n",
        "try:\n",
        "    # Use a standard sentence to warm up the model\n",
        "    warm_text = \"This is a warm-up sentence to prepare the model for inference.\"\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        # Step 1: Run the TTS model pipeline to get audio output\n",
        "        gen = pipeline(warm_text, voice=KOKORO_VOICE)\n",
        "        # The generator yields numpy arrays, so we concatenate them\n",
        "        audio_chunks = [audio_np for _, _, audio_np in gen]\n",
        "        full_audio_np = np.concatenate(audio_chunks)\n",
        "\n",
        "        # Step 2: Convert to a tensor and move to the GPU\n",
        "        wav24_tensor = torch.from_numpy(full_audio_np).float().to(device)\n",
        "\n",
        "        # Step 3: Warm up the resampler\n",
        "        wav44_tensor = resample_24k_to_44k(wav24_tensor)\n",
        "\n",
        "        # Step 4: Warm up the PCM/Base64 encoding step\n",
        "        _, _ = pcm16_base64_from_f32_cpu(wav44_tensor)\n",
        "\n",
        "        # Step 5: Warm up the alignment model\n",
        "        _ = aligner.create_alignment(\n",
        "            warm_text,\n",
        "            (wav44_tensor.numel() / OUT_SR) * 1000,\n",
        "            wav44_tensor\n",
        "        )\n",
        "\n",
        "    print(f\"‚úÖ Full pipeline warm-up successful in {int((time.time()-t0)*1000)} ms\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Warm-up failed: {e}\")\n",
        "    print(\"The server might have a higher latency on the first request.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdfthpdK94R-",
        "outputId": "e7d712da-e52e-49ba-e044-476e19f3d1ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî• Warming up the TTS pipeline...\n",
            "‚úÖ Full pipeline warm-up successful in 420 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 8: FastAPI App Setup ====\n",
        "from fastapi import FastAPI, WebSocket, Body\n",
        "from fastapi.responses import HTMLResponse, StreamingResponse\n",
        "from traceback import format_exc\n",
        "import time, json, torch, io, threading, queue as pyqueue\n",
        "import soundfile as sf\n",
        "import os\n",
        "import numpy as np\n",
        "import asyncio\n",
        "from collections import deque\n",
        "\n",
        "# Keep fastest algos once warmed\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Optimized parameters for Colab\n",
        "MIN_SEND_MS = 100\n",
        "PREBUFFER_MS = 200\n",
        "HEADROOM_MS = 150\n",
        "FIRST_BURST_MS = PREBUFFER_MS\n",
        "FALLBACK_CPS = 15.0\n",
        "\n",
        "MIN_SEND_SAMPLES = int(OUT_SR * (MIN_SEND_MS / 1000.0))\n",
        "FIRST_BURST_SAMPLES = int(OUT_SR * (FIRST_BURST_MS / 1000.0))\n",
        "\n",
        "app = FastAPI(allow_hosts=['*'])\n",
        "\n",
        "# --- Embedded HTML and JavaScript ---\n",
        "HTML = f\"\"\"\n",
        "<!doctype html>\n",
        "<meta charset=\"utf-8\" />\n",
        "<title>Kokoro Realtime TTS</title>\n",
        "<style>\n",
        "  body {{ font: 14px/1.55 system-ui, -apple-system, Segoe UI, Roboto, sans-serif; max-width: 920px; margin: 32px auto; }}\n",
        "  textarea {{ width:100%; height:140px; font: 15px/1.45 ui-monospace, Menlo, Consolas, monospace; }}\n",
        "  button {{ padding:8px 14px; border-radius:10px; border:1px solid #ccc; background:#fff; cursor:pointer; }}\n",
        "  .row {{ display:flex; gap:10px; align-items:center; margin:10px 0; flex-wrap: wrap; }}\n",
        "  #status {{ color:#666; }}\n",
        "  #captions {{ margin-top:12px; font-size:20px; white-space: pre-wrap; }}\n",
        "  .hl {{ background: #ffe58a; }}\n",
        "  #err {{ margin-top:8px; color:#b00020; font-family: ui-monospace, Menlo, Consolas, monospace; white-space: pre-wrap; }}\n",
        "  #cfg {{ font-size:12px; color:#666; }}\n",
        "</style>\n",
        "<div class=\"row\">\n",
        "  <button id=\"connect\">Connect</button>\n",
        "  <button id=\"disconnect\" disabled>Disconnect</button>\n",
        "  <button id=\"tone\" disabled>Test tone</button>\n",
        "  <div id=\"status\">disconnected</div>\n",
        "</div>\n",
        "<div id=\"cfg\">\n",
        "  PREBUFFER={PREBUFFER_MS} ms ¬∑ HEADROOM={HEADROOM_MS} ms ¬∑ MIN_SEND={MIN_SEND_MS} ms ¬∑ FIRST_BURST={FIRST_BURST_SAMPLES} ms\n",
        "</div>\n",
        "\n",
        "<textarea id=\"input\" placeholder=\"Type text. Flush to speak; End flushes remaining + closes.\" autofocus></textarea>\n",
        "<div class=\"row\">\n",
        "  <button id=\"flush\" disabled>Flush</button>\n",
        "  <button id=\"end\" disabled>End</button>\n",
        "</div>\n",
        "\n",
        "<div id=\"latency\"></div>\n",
        "<h3>Live captions</h3>\n",
        "<div id=\"captions\"></div>\n",
        "<pre id=\"err\"></pre>\n",
        "\n",
        "<script>\n",
        "let ws = null;\n",
        "let accText = \"\";\n",
        "let audioCtx = null;\n",
        "let queue = [];\n",
        "let queueSamples = 0;\n",
        "let started = false;\n",
        "let playHead = 0;\n",
        "let flushStartT = 0;\n",
        "const PREBUFFER_MS = {PREBUFFER_MS};\n",
        "const HEADROOM_MS = {HEADROOM_MS};\n",
        "\n",
        "// --- Global state for captions and a single timer ---\n",
        "let spans = [];\n",
        "let captionData = []; // To hold all character timing data\n",
        "let captionTimer = null;\n",
        "let drainTimer = null;\n",
        "\n",
        "\n",
        "function startDrainLoop() {{\n",
        "  if (drainTimer) return;\n",
        "  drainTimer = setInterval(() => {{\n",
        "    if (queue.length > 0 || started) drainQueue();\n",
        "  }}, 25);\n",
        "}}\n",
        "\n",
        "function stopDrainLoop() {{\n",
        "  if (drainTimer) {{ clearInterval(drainTimer); drainTimer = null; }}\n",
        "}}\n",
        "\n",
        "function ensureCtx() {{\n",
        "  if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)({{ sampleRate: 44100 }});\n",
        "  if (audioCtx.state === \"suspended\") audioCtx.resume();\n",
        "}}\n",
        "\n",
        "function primeAudio() {{\n",
        "  ensureCtx();\n",
        "  const o = audioCtx.createOscillator(); const g = audioCtx.createGain(); g.gain.value = 0.0001;\n",
        "  o.connect(g); g.connect(audioCtx.destination); o.start(); o.stop(audioCtx.currentTime + 0.02);\n",
        "}}\n",
        "\n",
        "function testTone() {{\n",
        "  ensureCtx();\n",
        "  const o = audioCtx.createOscillator(); const g = audioCtx.createGain(); g.gain.value = 0.08;\n",
        "  o.frequency.value = 440; o.connect(g); g.connect(audioCtx.destination);\n",
        "  o.start(); o.stop(audioCtx.currentTime + 0.8);\n",
        "}}\n",
        "\n",
        "function resetPlaybackState() {{\n",
        "    queue = [];\n",
        "    queueSamples = 0;\n",
        "    started = false;\n",
        "    playHead = 0;\n",
        "    flushStartT = 0;\n",
        "\n",
        "    // --- Clear caption state on reset ---\n",
        "    spans = [];\n",
        "    captionData = [];\n",
        "    if (captionTimer) {{\n",
        "        clearInterval(captionTimer);\n",
        "        captionTimer = null;\n",
        "    }}\n",
        "    const cap = document.getElementById(\"captions\");\n",
        "    if(cap) cap.textContent = \"\";\n",
        "}}\n",
        "\n",
        "function pushPCM16(b64) {{\n",
        "  const b = atob(b64);\n",
        "  const arr = new Int16Array(b.length/2);\n",
        "  for (let i=0;i<arr.length;i++) arr[i] = (b.charCodeAt(2*i) | (b.charCodeAt(2*i+1) << 8)) << 16 >> 16;\n",
        "  queue.push(arr); queueSamples += arr.length; drainQueue();\n",
        "}}\n",
        "\n",
        "function bufferedMs() {{ return (queueSamples / 44100) * 1000; }}\n",
        "\n",
        "function startCaptionTimer() {{\n",
        "    if (captionTimer) return; // Ensure only one timer is running\n",
        "\n",
        "    captionTimer = setInterval(() => {{\n",
        "        if (!audioCtx || !flushStartT) return;\n",
        "\n",
        "        const elapsedMs = (audioCtx.currentTime - flushStartT) * 1000;\n",
        "\n",
        "        captionData.forEach(item => {{\n",
        "            const isHighlighted = item.span.classList.contains(\"hl\");\n",
        "            const shouldBeHighlighted = elapsedMs >= item.start && elapsedMs < item.end;\n",
        "\n",
        "            if (isHighlighted && !shouldBeHighlighted) {{\n",
        "                item.span.classList.remove(\"hl\");\n",
        "            }} else if (!isHighlighted && shouldBeHighlighted) {{\n",
        "                item.span.classList.add(\"hl\");\n",
        "            }}\n",
        "        }});\n",
        "\n",
        "        if (audioCtx.currentTime > playHead + 0.5) {{\n",
        "            clearInterval(captionTimer);\n",
        "            captionTimer = null;\n",
        "        }}\n",
        "    }}, 50); // Update captions ~20 times per second\n",
        "}}\n",
        "\n",
        "function drainQueue() {{\n",
        "  ensureCtx();\n",
        "  if (!started) {{\n",
        "    if (bufferedMs() < PREBUFFER_MS) return;\n",
        "    started = true;\n",
        "    playHead = audioCtx.currentTime;\n",
        "\n",
        "    startCaptionTimer();\n",
        "  }}\n",
        "\n",
        "  while (queue.length > 0) {{\n",
        "    const aheadMs = (playHead - audioCtx.currentTime) * 1000;\n",
        "    if (aheadMs > HEADROOM_MS) break;\n",
        "\n",
        "    const int16 = queue.shift();\n",
        "    queueSamples -= int16.length;\n",
        "\n",
        "    const buf = audioCtx.createBuffer(1, int16.length, 44100);\n",
        "    const f32 = new Float32Array(int16.length);\n",
        "    for (let i = 0; i < int16.length; i++) f32[i] = Math.max(-1, Math.min(1, int16[i] / 32768));\n",
        "    buf.getChannelData(0).set(f32);\n",
        "\n",
        "    const src = audioCtx.createBufferSource();\n",
        "    src.buffer = buf;\n",
        "    src.connect(audioCtx.destination);\n",
        "\n",
        "    const startAt = Math.max(playHead, audioCtx.currentTime + 0.02);\n",
        "    if (flushStartT === 0) {{\n",
        "        flushStartT = startAt;\n",
        "    }}\n",
        "\n",
        "    src.start(startAt);\n",
        "    playHead = startAt + buf.duration;\n",
        "  }}\n",
        "}}\n",
        "\n",
        "function updateCaptions(chars, starts, durs, offsetMs) {{\n",
        "    const cap = document.getElementById(\"captions\");\n",
        "    offsetMs = offsetMs || 0;\n",
        "\n",
        "    chars.forEach((char, i) => {{\n",
        "        const span = document.createElement(\"span\");\n",
        "        span.textContent = char;\n",
        "        cap.appendChild(span);\n",
        "        spans.push(span);\n",
        "\n",
        "        captionData.push({{\n",
        "            start: starts[i] + offsetMs,\n",
        "            end: starts[i] + offsetMs + durs[i],\n",
        "            span: span\n",
        "        }});\n",
        "    }});\n",
        "}}\n",
        "\n",
        "function wireWSHandlers() {{\n",
        "  ws.onopen = () => {{\n",
        "    document.getElementById(\"status\").textContent = \"connected\";\n",
        "    document.getElementById(\"connect\").disabled = true;\n",
        "    document.getElementById(\"disconnect\").disabled = false;\n",
        "    document.getElementById(\"flush\").disabled = false;\n",
        "    document.getElementById(\"end\").disabled = false;\n",
        "    document.getElementById(\"tone\").disabled = false;\n",
        "    document.getElementById(\"err\").textContent = \"\";\n",
        "    ws.send(JSON.stringify({{text: \" \", flush: false}}));\n",
        "    startDrainLoop();\n",
        "  }};\n",
        "\n",
        "  ws.onmessage = (ev) => {{\n",
        "    const msg = JSON.parse(ev.data);\n",
        "    // DEBUGGING: This will show you exactly what the server sends in your browser's console (F12)\n",
        "    console.log(\"Received chunk:\", msg);\n",
        "\n",
        "    if (msg.error) {{\n",
        "      console.error(\"Server error:\", msg.error);\n",
        "      document.getElementById(\"err\").textContent = msg.error;\n",
        "      if (ws) ws.close(1000, \"Server error\");\n",
        "      return;\n",
        "    }}\n",
        "    if (msg.meta && msg.meta.first_chunk_ms !== undefined) {{\n",
        "      document.getElementById(\"latency\").textContent = \"First-chunk latency: \" + msg.meta.first_chunk_ms + \" ms\";\n",
        "    }}\n",
        "    if (msg.audio) pushPCM16(msg.audio);\n",
        "\n",
        "    if (msg.alignment && msg.alignment.chars) {{\n",
        "      const a = msg.alignment;\n",
        "      updateCaptions(a.chars, a.char_start_times_ms, a.char_durations_ms, a.offset_ms);\n",
        "    }}\n",
        "  }};\n",
        "\n",
        "  const cleanup = () => {{\n",
        "    document.getElementById(\"status\").textContent = \"disconnected\";\n",
        "    document.getElementById(\"connect\").disabled = false;\n",
        "    document.getElementById(\"disconnect\").disabled = true;\n",
        "    document.getElementById(\"flush\").disabled = true;\n",
        "    document.getElementById(\"end\").disabled = true;\n",
        "    document.getElementById(\"tone\").disabled = true;\n",
        "    resetPlaybackState();\n",
        "    stopDrainLoop();\n",
        "    ws = null;\n",
        "  }};\n",
        "\n",
        "  ws.onclose = cleanup;\n",
        "  ws.onerror = (e) => {{\n",
        "    console.error(\"WS error\", e);\n",
        "    cleanup();\n",
        "  }};\n",
        "}}\n",
        "\n",
        "document.getElementById(\"connect\").onclick = () => {{\n",
        "  primeAudio();\n",
        "  if (ws) return;\n",
        "  const proto = location.protocol === \"https:\" ? \"wss\" : \"ws\";\n",
        "  ws = new WebSocket(`${{proto}}://${{location.host}}/ws`);\n",
        "  wireWSHandlers();\n",
        "}};\n",
        "\n",
        "document.getElementById(\"disconnect\").onclick = () => {{ if (ws) ws.close(1000, \"client disconnect\"); }};\n",
        "\n",
        "document.getElementById(\"tone\").onclick = () => testTone();\n",
        "\n",
        "document.getElementById(\"flush\").onclick = () => {{\n",
        "  const ta = document.getElementById(\"input\");\n",
        "  const newText = ta.value.slice(accText.length);\n",
        "  accText = ta.value;\n",
        "  resetPlaybackState();\n",
        "  ws && ws.send(JSON.stringify({{text: newText, flush: true}}));\n",
        "  startDrainLoop();\n",
        "}};\n",
        "\n",
        "document.getElementById(\"end\").onclick = () => {{\n",
        "  const ta = document.getElementById(\"input\");\n",
        "  const pending = ta.value.slice(accText.length);\n",
        "  accText = ta.value;\n",
        "  ws && ws.send(JSON.stringify({{text: pending, flush: true, end: true}}));\n",
        "}};\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "@app.get(\"/\")\n",
        "def index():\n",
        "    return HTMLResponse(HTML)\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    ok = bool('pipeline' in globals()) and (OUT_SR == 44100) and (MODEL_SR == 24000) and ('KOKORO_VOICE' in globals()) and ('SPEAKER_READY' in globals())\n",
        "    return {\"ok\": ok, \"device\": device,\n",
        "            \"server_min_send_ms\": MIN_SEND_MS, \"first_burst_ms\": FIRST_BURST_SAMPLES}\n",
        "\n",
        "\n",
        "@app.websocket(\"/ws\")\n",
        "async def ws_endpoint_minimal_latency(ws: WebSocket):\n",
        "    await ws.accept()\n",
        "    text_buffer = \"\"\n",
        "    synthesized_len = 0\n",
        "\n",
        "    try:\n",
        "        assert 'SPEAKER_READY' in globals() and SPEAKER_READY, \"Kokoro setup incomplete.\"\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                raw = await ws.receive_text()\n",
        "                data = json.loads(raw)\n",
        "            except Exception:\n",
        "                # Client disconnected\n",
        "                break\n",
        "\n",
        "            text = data.get(\"text\", \"\")\n",
        "            flush = data.get(\"flush\", False)\n",
        "            end = data.get(\"end\", False)\n",
        "\n",
        "            if text == \" \":\n",
        "                continue\n",
        "\n",
        "            text_buffer += text\n",
        "            should_flush_now = flush or (end and synthesized_len < len(text_buffer))\n",
        "\n",
        "            if should_flush_now:\n",
        "                new_text = text_buffer[synthesized_len:]\n",
        "                if not new_text.strip():\n",
        "                    if end:\n",
        "                        await ws.close()\n",
        "                        break\n",
        "                    continue\n",
        "\n",
        "                new_text_clean = speakable_math(new_text)\n",
        "                t0 = time.time()\n",
        "                q = pyqueue.Queue()\n",
        "\n",
        "                def producer():\n",
        "                    try:\n",
        "                        with torch.inference_mode():\n",
        "                            # 1. Generate the entire audio clip at once\n",
        "                            generator = pipeline(new_text_clean, voice=KOKORO_VOICE)\n",
        "                            full_audio_np_list = [audio_np for _, _, audio_np in generator]\n",
        "                            if not full_audio_np_list:\n",
        "                                q.put({\"type\": \"done_empty\"})\n",
        "                                return\n",
        "\n",
        "                            full_audio_np = np.concatenate(full_audio_np_list)\n",
        "                            wav24_tensor = torch.from_numpy(full_audio_np).float().to(device)\n",
        "\n",
        "                            # 2. Resample ONCE\n",
        "                            full_audio_44k = resample_24k_to_44k(wav24_tensor)\n",
        "                            audio_duration_ms = (full_audio_44k.numel() / OUT_SR) * 1000\n",
        "\n",
        "                            # 3. Generate the full alignment data\n",
        "                            alignment = aligner.create_alignment(\n",
        "                                new_text_clean, audio_duration_ms, full_audio_44k\n",
        "                            )\n",
        "                            # 4. Put the complete results in the queue for the sender to slice\n",
        "                            q.put({\n",
        "                                \"type\": \"generated\",\n",
        "                                \"audio\": full_audio_44k,\n",
        "                                \"alignment\": alignment\n",
        "                            })\n",
        "                    except Exception as e:\n",
        "                        q.put({\"type\": \"error\", \"err\": format_exc()})\n",
        "\n",
        "                th = threading.Thread(target=producer, daemon=True)\n",
        "                th.start()\n",
        "\n",
        "                item = q.get() # Wait for the producer to finish\n",
        "\n",
        "                if item[\"type\"] == \"error\":\n",
        "                    await ws.send_json({\"error\": item[\"err\"]})\n",
        "                elif item[\"type\"] == \"done_empty\":\n",
        "                    pass # Nothing to send\n",
        "                elif item[\"type\"] == \"generated\":\n",
        "                    full_audio_44k = item[\"audio\"]\n",
        "                    alignment_data = item[\"alignment\"]\n",
        "                    first_chunk_sent = False\n",
        "\n",
        "                    audio_offset_ms = 0\n",
        "                    char_cursor = 0\n",
        "                    res_all = full_audio_44k\n",
        "\n",
        "                    while res_all.numel() > 0:\n",
        "                        # 1. Determine the size of the next audio chunk\n",
        "                        want_samples = FIRST_BURST_SAMPLES if not first_chunk_sent else MIN_SEND_SAMPLES\n",
        "                        take_samples = min(want_samples, res_all.numel())\n",
        "                        chunk_tensor = res_all[:take_samples]\n",
        "                        res_all = res_all[take_samples:]\n",
        "                        chunk_duration_ms = (chunk_tensor.numel() / OUT_SR) * 1000\n",
        "\n",
        "                        # 2. Encode the audio chunk\n",
        "                        b64, _ = pcm16_base64_from_f32_cpu(chunk_tensor)\n",
        "\n",
        "                        # 3. Find which characters belong to this audio chunk\n",
        "                        chunk_chars, chunk_starts, chunk_durs = [], [], []\n",
        "                        while (char_cursor < len(alignment_data[\"chars\"]) and\n",
        "                               alignment_data[\"char_start_times_ms\"][char_cursor] < audio_offset_ms + chunk_duration_ms):\n",
        "\n",
        "                            char_start_global = alignment_data[\"char_start_times_ms\"][char_cursor]\n",
        "                            char_dur = alignment_data[\"char_durations_ms\"][char_cursor]\n",
        "\n",
        "                            chunk_chars.append(alignment_data[\"chars\"][char_cursor])\n",
        "                            # Make start time relative to the chunk's start\n",
        "                            chunk_starts.append(char_start_global - audio_offset_ms)\n",
        "                            chunk_durs.append(char_dur)\n",
        "                            char_cursor += 1\n",
        "\n",
        "                        # 4. Assemble and send the combined payload\n",
        "                        payload = {\n",
        "                            \"audio\": b64,\n",
        "                            \"alignment\": {\n",
        "                                \"chars\": chunk_chars,\n",
        "                                \"char_start_times_ms\": chunk_starts,\n",
        "                                \"char_durations_ms\": chunk_durs,\n",
        "                                \"offset_ms\": audio_offset_ms\n",
        "                            }\n",
        "                        }\n",
        "                        if not first_chunk_sent:\n",
        "                            payload[\"meta\"] = {\"first_chunk_ms\": int((time.time() - t0) * 1000)}\n",
        "                            first_chunk_sent = True\n",
        "\n",
        "                        await ws.send_json(payload)\n",
        "                        audio_offset_ms += chunk_duration_ms\n",
        "\n",
        "                synthesized_len = len(text_buffer)\n",
        "\n",
        "            if end:\n",
        "                await ws.close()\n",
        "                break\n",
        "\n",
        "    except Exception:\n",
        "        err = format_exc()\n",
        "        print(\"WebSocket error:\", err)\n",
        "        try:\n",
        "            await ws.send_json({\"error\": err})\n",
        "            await ws.close()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "@app.post(\"/synth_wav\")\n",
        "def synth_wav(payload: dict = Body(...)):\n",
        "    text = (payload or {}).get(\"text\", \"\").strip() or \"Hello from XTTS.\"\n",
        "    assert 'SPEAKER_READY' in globals() and SPEAKER_READY, \"Speaker latents missing ‚Äî re-run the upload cell.\"\n",
        "\n",
        "    generator = pipeline(text, voice=KOKORO_VOICE)\n",
        "    full_audio = np.concatenate([audio_np for _, _, audio_np in generator])\n",
        "\n",
        "    wav24 = torch.from_numpy(full_audio).float()\n",
        "    res = resample_24k_to_44k(wav24).numpy()\n",
        "    buf = io.BytesIO(); sf.write(buf, res, OUT_SR, format=\"WAV\", subtype=\"PCM_16\"); buf.seek(0)\n",
        "    return StreamingResponse(buf, media_type=\"audio/wav\")"
      ],
      "metadata": {
        "id": "rylSivo2EDMX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f uvicorn -9\n",
        "!pkill -f ngrok -9"
      ],
      "metadata": {
        "id": "BqCoQrzIrraz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 10: Performance Monitoring ====\n",
        "import threading\n",
        "import time\n",
        "import psutil\n",
        "import gc\n",
        "\n",
        "class PerformanceMonitor:\n",
        "    def __init__(self):\n",
        "        self.is_running = False\n",
        "        # --- FIX: Removed \"active_sessions\" which was causing an error ---\n",
        "        self.stats = {\n",
        "            \"gpu_memory_mb\": 0,\n",
        "            \"cpu_percent\": 0,\n",
        "            \"ram_percent\": 0,\n",
        "            \"total_requests\": 0\n",
        "        }\n",
        "\n",
        "    def start_monitoring(self):\n",
        "        if self.is_running:\n",
        "            return\n",
        "\n",
        "        self.is_running = True\n",
        "        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)\n",
        "        self.monitor_thread.start()\n",
        "        print(\"Performance monitoring started\")\n",
        "\n",
        "    def stop_monitoring(self):\n",
        "        self.is_running = False\n",
        "\n",
        "    def _monitor_loop(self):\n",
        "        while self.is_running:\n",
        "            try:\n",
        "                # GPU memory\n",
        "                if torch.cuda.is_available():\n",
        "                    gpu_mem = torch.cuda.memory_allocated() / (1024**2) # MB\n",
        "                    self.stats[\"gpu_memory_mb\"] = int(gpu_mem)\n",
        "\n",
        "                # CPU and RAM\n",
        "                self.stats[\"cpu_percent\"] = psutil.cpu_percent(interval=None)\n",
        "                self.stats[\"ram_percent\"] = psutil.virtual_memory().percent\n",
        "\n",
        "                # --- FIX: Removed all logic related to the non-existent 'sessions' variable ---\n",
        "\n",
        "                # Force garbage collection periodically\n",
        "                if self.stats[\"total_requests\"] > 0 and self.stats[\"total_requests\"] % 10 == 0:\n",
        "                    gc.collect()\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Monitoring error: {e}\")\n",
        "\n",
        "            time.sleep(5) # Update every 5 seconds\n",
        "\n",
        "# Global performance monitor\n",
        "perf_monitor = PerformanceMonitor()\n",
        "\n",
        "@app.get(\"/stats\")\n",
        "def get_stats():\n",
        "    \"\"\"Get current performance statistics\"\"\"\n",
        "    return perf_monitor.stats\n",
        "\n",
        "@app.post(\"/gc\")\n",
        "def force_garbage_collection():\n",
        "    \"\"\"Force garbage collection - useful for freeing GPU memory\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    return {\"status\": \"garbage collection completed\"}\n",
        "\n",
        "print(\"Performance monitoring ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc9D4PAxrcJy",
        "outputId": "2337426b-255c-4572-a7a3-96837023f1d6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance monitoring ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== Cell 11: Fixed Server Startup with ngrok ====\n",
        "import uvicorn\n",
        "import threading\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "import requests\n",
        "import socket\n",
        "import subprocess\n",
        "\n",
        "def kill_existing_servers():\n",
        "    \"\"\"Kill any existing servers and ngrok processes\"\"\"\n",
        "    try:\n",
        "        # Kill uvicorn processes\n",
        "        subprocess.run([\"pkill\", \"-f\", \"uvicorn\"], check=False)\n",
        "        # Kill ngrok processes\n",
        "        ngrok.kill()\n",
        "        time.sleep(3)\n",
        "        print(\"Cleaned up existing processes\")\n",
        "    except Exception as e:\n",
        "        print(f\"Cleanup warning: {e}\")\n",
        "\n",
        "def find_free_port():\n",
        "    \"\"\"Find a free port to use\"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind(('', 0))\n",
        "        s.listen(1)\n",
        "        port = s.getsockname()[1]\n",
        "    return port\n",
        "\n",
        "def start_server(port=None):\n",
        "    \"\"\"Start the FastAPI server in a separate thread\"\"\"\n",
        "    if port is None:\n",
        "        port = find_free_port()\n",
        "\n",
        "    def run_server():\n",
        "        try:\n",
        "            config = uvicorn.Config(\n",
        "                app=app,\n",
        "                host=\"0.0.0.0\",\n",
        "                port=port,\n",
        "                log_level=\"info\",\n",
        "                access_log=True, # Enable for debugging\n",
        "                ws_ping_interval=20, # WebSocket keepalive\n",
        "                ws_ping_timeout=20\n",
        "            )\n",
        "            server = uvicorn.Server(config)\n",
        "            server.run()\n",
        "        except Exception as e:\n",
        "            print(f\"Server error: {e}\")\n",
        "\n",
        "    server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "    server_thread.start()\n",
        "    return server_thread, port\n",
        "\n",
        "def setup_ngrok(port):\n",
        "    \"\"\"Setup ngrok tunnel for external access\"\"\"\n",
        "    try:\n",
        "        # Kill any existing tunnels\n",
        "        ngrok.kill()\n",
        "        time.sleep(2)\n",
        "\n",
        "        # Set auth token if needed\n",
        "        try:\n",
        "            ngrok.set_auth_token(\"32MTcbnY6MqXuSmULcm2ujQJxbU_78APC8wmkMAC9HKEWAh8e\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Create HTTP tunnel with WebSocket support\n",
        "        tunnel = ngrok.connect(port, bind_tls=True)\n",
        "        public_url = tunnel.public_url\n",
        "\n",
        "        print(f\"\\n‚úÖ Public URL: {public_url}\")\n",
        "        print(f\"üì± Direct link: {public_url}\")\n",
        "\n",
        "        # Test health endpoint\n",
        "        time.sleep(3)\n",
        "        try:\n",
        "            # Test HTTP endpoint\n",
        "            health_response = requests.get(f\"{public_url}/health\", timeout=10)\n",
        "            if health_response.status_code == 200:\n",
        "                print(\"‚úÖ Health check passed\")\n",
        "                health_data = health_response.json()\n",
        "                print(f\"   Device: {health_data.get('device', 'unknown')}\")\n",
        "                print(f\"   Streaming: {health_data.get('streaming', False)}\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Health check returned: {health_response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Health check error: {e}\")\n",
        "\n",
        "        return public_url\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ngrok setup failed: {e}\")\n",
        "        print(f\"Server available locally at: http://localhost:{port}\")\n",
        "        return None\n",
        "\n",
        "# Clean up first\n",
        "print(\"üßπ Cleaning up existing servers...\")\n",
        "kill_existing_servers()\n",
        "\n",
        "# Find a free port\n",
        "port = find_free_port()\n",
        "print(f\"üìç Using port: {port}\")\n",
        "\n",
        "# Start the server\n",
        "print(\"üöÄ Starting TTS server...\")\n",
        "server_thread, server_port = start_server(port)\n",
        "\n",
        "# Wait for server to fully start\n",
        "print(\"‚è≥ Waiting for server initialization...\")\n",
        "time.sleep(5)\n",
        "\n",
        "# Verify server is running locally first\n",
        "try:\n",
        "    local_test = requests.get(f\"http://localhost:{server_port}/health\", timeout=5)\n",
        "    if local_test.status_code == 200:\n",
        "        print(\"‚úÖ Local server is running\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Local server issue detected\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Local server not responding: {e}\")\n",
        "\n",
        "# Start performance monitoring\n",
        "perf_monitor.start_monitoring()\n",
        "\n",
        "# Setup ngrok tunnel\n",
        "public_url = setup_ngrok(server_port)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéô Kokoro Real-time TTS Server is Ready!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if public_url:\n",
        "    print(f\"üåê Public URL: {public_url}\")\n",
        "    print(f\"üì± Mobile/External: {public_url}\")\n",
        "    print(\"\\n‚ö†Ô∏è IMPORTANT: If you see 'Ngrok Visit Site' page:\")\n",
        "    print(\"   Click 'Visit Site' to access your application\")\n",
        "else:\n",
        "    print(f\"üè† Local URL: http://localhost:{server_port}\")\n",
        "\n",
        "print(\"\\nüìä Endpoints:\")\n",
        "print(f\"   Main interface: {public_url if public_url else f'http://localhost:{server_port}'}\")\n",
        "print(f\"   Stats: {public_url if public_url else f'http://localhost:{server_port}'}/stats\")\n",
        "print(f\"   Health: {public_url if public_url else f'http://localhost:{server_port}'}/health\")\n",
        "\n",
        "print(\"\\nüí° Troubleshooting:\")\n",
        "print(\"  1. If 'Connect' doesn't work, check browser console (F12)\")\n",
        "print(\"  2. Try refreshing the page after clicking 'Visit Site' on ngrok\")\n",
        "print(\"  3. Ensure pop-ups are allowed for ngrok domain\")\n",
        "print(\"  4. Test with 'Test Audio' button first\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"‚úÖ Server is running. Access the interface via the URL above!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6LD2Px6rf8P",
        "outputId": "fef332ea-47a4-4715-ff00-d45b52e3927b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Cleaning up existing servers...\n",
            "Cleaned up existing processes\n",
            "üìç Using port: 60213\n",
            "üöÄ Starting TTS server...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [272]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:60213 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Waiting for server initialization...\n",
            "INFO:     127.0.0.1:59774 - \"GET /health HTTP/1.1\" 200 OK\n",
            "‚úÖ Local server is running\n",
            "Performance monitoring started\n",
            "\n",
            "‚úÖ Public URL: https://4a7a141ca97b.ngrok-free.app\n",
            "üì± Direct link: https://4a7a141ca97b.ngrok-free.app\n",
            "INFO:     34.82.221.37:0 - \"GET /health HTTP/1.1\" 200 OK\n",
            "‚úÖ Health check passed\n",
            "   Device: cuda\n",
            "   Streaming: False\n",
            "\n",
            "============================================================\n",
            "üéô Kokoro Real-time TTS Server is Ready!\n",
            "============================================================\n",
            "üåê Public URL: https://4a7a141ca97b.ngrok-free.app\n",
            "üì± Mobile/External: https://4a7a141ca97b.ngrok-free.app\n",
            "\n",
            "‚ö†Ô∏è IMPORTANT: If you see 'Ngrok Visit Site' page:\n",
            "   Click 'Visit Site' to access your application\n",
            "\n",
            "üìä Endpoints:\n",
            "   Main interface: https://4a7a141ca97b.ngrok-free.app\n",
            "   Stats: https://4a7a141ca97b.ngrok-free.app/stats\n",
            "   Health: https://4a7a141ca97b.ngrok-free.app/health\n",
            "\n",
            "üí° Troubleshooting:\n",
            "  1. If 'Connect' doesn't work, check browser console (F12)\n",
            "  2. Try refreshing the page after clicking 'Visit Site' on ngrok\n",
            "  3. Ensure pop-ups are allowed for ngrok domain\n",
            "  4. Test with 'Test Audio' button first\n",
            "============================================================\n",
            "‚úÖ Server is running. Access the interface via the URL above!\n"
          ]
        }
      ]
    }
  ]
}